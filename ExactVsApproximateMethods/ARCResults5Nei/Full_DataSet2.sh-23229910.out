
R version 4.2.1 (2022-06-23) -- "Funny-Looking Kid"
Copyright (C) 2022 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> rm(list=ls())
> graphics.off()
> library(fields)
Loading required package: spam
Spam version 2.9-1 (2022-08-07) is loaded.
Type 'help( Spam)' or 'demo( spam)' for a short introduction 
and overview of this package.
Help for individual functions is also obtained by adding the
suffix '.spam' to the function name, e.g. 'help( chol.spam)'.

Attaching package: ‘spam’

The following objects are masked from ‘package:base’:

    backsolve, forwardsolve

Loading required package: viridis
Loading required package: viridisLite

Try help(fields) to get started.
> library(Matrix)

Attaching package: ‘Matrix’

The following object is masked from ‘package:spam’:

    det

> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.2     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.2     ✔ tibble    3.2.1
✔ lubridate 1.9.2     ✔ tidyr     1.3.0
✔ purrr     1.0.1     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ tidyr::expand() masks Matrix::expand()
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ tidyr::pack()   masks Matrix::pack()
✖ tidyr::unpack() masks Matrix::unpack()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(magrittr)

Attaching package: ‘magrittr’

The following object is masked from ‘package:purrr’:

    set_names

The following object is masked from ‘package:tidyr’:

    extract

> library(tidybayes)
> library(coda)
> library(nleqslv)
> 
> fpath <- "/home/ParitoshKRoy/git/ApproximateGLGC/"
> fpath <- "/home/pkroy/projects/def-aschmidt/pkroy/ApproximateGLGC/" #@ARC
> 
> source(paste0(fpath,"Rutilities/utility_functions.R"))
> source(paste0(fpath,"ExactVsApproximateMethods/gen_data_set2.R"))
> 
> # partition as observed and predicted
> obsCoords <- coords[idSampled,]
> prdCoords <- coords[-idSampled,]
> obsY <- y[idSampled]
> prdY <- y[-idSampled]
> obsX <- X[idSampled,]
> prdX <- X[-idSampled,]
> obsZ1 <- z1[idSampled]
> obsZ2 <- z2[idSampled]
> prdZ1 <- z1[-idSampled]
> prdZ2 <- z2[-idSampled]
> 
> obsDistMat <- fields::rdist(obsCoords)
> str(obsDistMat)
 num [1:500, 1:500] 0 0.752 1.208 0.58 0.279 ...
> obsDistVec <- obsDistMat[lower.tri(obsDistMat, diag = FALSE)]
> obsMaxDist <- max(obsDistVec)
> obsMedDist <- median(obsDistVec)
> obsMinDist <- min(obsDistVec)
> rm(obsDistMat)
> 
> ## Prior elicitation
> lLimit <- quantile(obsDistVec, prob = 0.01); lLimit
      1% 
0.116619 
> uLimit <- quantile(obsDistVec, prob = 0.99); uLimit
     99% 
2.161481 
> 
> library(nleqslv)
> ab <- nleqslv(c(5,0.1), getIGamma, lRange = lLimit, uRange = uLimit, prob = 0.98)$x
> ab
[1] 3.0682196 0.9937193
> curve(dinvgamma(x, shape = ab[1], scale = ab[2]), 0, uLimit)
> summary(rinvgamma(n = 1000, shape = ab[1], scale = ab[2]))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.09135 0.24779 0.35998 0.47242 0.56510 6.43102 
> 
> ## Exponential and PC prior
> lambda_sigma1 <- -log(0.01)/1; lambda_sigma1
[1] 4.60517
> lambda_sigma2 <- -log(0.01)/1; lambda_sigma2
[1] 4.60517
> lambda_tau <- -log(0.01)/1; lambda_tau
[1] 4.60517
> pexp(q = 1, rate = lambda_tau, lower.tail = TRUE) ## P(tau > 1) = 0.05
[1] 0.99
> lambda_ell1 <- as.numeric(-log(0.01)*lLimit); lambda_ell1
[1] 0.5370505
> lambda_ell2 <- as.numeric(-log(0.01)*lLimit); lambda_ell2
[1] 0.5370505
> pfrechet(q = lLimit, alpha = 1, sigma = lambda_ell2, lower.tail = TRUE) ## P(ell < lLimit) = 0.05
  1% 
0.01 
> summary(rfrechet(n = 1000, alpha = 1, sigma = lambda_ell2))
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
    0.059     0.387     0.763    34.730     1.844 27034.593 
> 
> ## Stan input
> P <- 3
> mu_theta <- c(mean(obsY),rep(0, P-1))
> V_theta <- diag(c(10,rep(1,P-1)))
> input <- list(N = nsize, P = P, y = obsY, X = obsX, coords = obsCoords, mu_theta = mu_theta, V_theta = V_theta, lambda_sigma1 = lambda_sigma1, lambda_sigma2 = lambda_sigma2, lambda_tau = lambda_tau, a = ab[1], b = ab[2], lambda_ell1 = lambda_ell1, lambda_ell2 = lambda_ell2, positive_skewness = 0)
> str(input)
List of 15
 $ N                : num 500
 $ P                : num 3
 $ y                : num [1:500] 0.785 3.29 1.626 0.934 -4.758 ...
 $ X                : num [1:500, 1:3] 1 1 1 1 1 1 1 1 1 1 ...
 $ coords           : num [1:500, 1:2] -0.77 -0.09 0.41 -0.19 -0.51 -0.11 0.53 0.23 0.93 0.79 ...
 $ mu_theta         : num [1:3] 2.32 0 0
 $ V_theta          : num [1:3, 1:3] 10 0 0 0 1 0 0 0 1
 $ lambda_sigma1    : num 4.61
 $ lambda_sigma2    : num 4.61
 $ lambda_tau       : num 4.61
 $ a                : num 3.07
 $ b                : num 0.994
 $ lambda_ell1      : num 0.537
 $ lambda_ell2      : num 0.537
 $ positive_skewness: num 0
> 
> library(cmdstanr)
This is cmdstanr version 0.5.3
- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr
- CmdStan path: /home/pkroy/.cmdstan/cmdstan-2.33.1
- CmdStan version: 2.33.1
> stan_file <- paste0(fpath,"StanFiles/Full_GLGC_HN.stan")
> mod <- cmdstan_model(stan_file, compile = TRUE)
> mod$check_syntax(pedantic = TRUE)
Warning: The parameter ell2 has no priors. This means either no prior is
    provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.
Warning: The parameter ell1 has no priors. This means either no prior is
    provided, or the prior(s) depend on data variables. In the later case,
    this may be a false positive.

Stan program is syntactically correct
> mod$print()
functions{ // functions starts

matrix my_gp_matern32_cov(array[] vector x, array[] vector y, real sigma, real lscale){
  return gp_matern32_cov(x, y, sigma, lscale);
}

/* stan function for backsolve*/
vector mdivide_left_tri_upp(matrix U, vector b) { 
  int n = rows(U);
  vector[n] x = b; 
  real cs;
  array[n] int ids = sort_desc(linspaced_int_array(n, 1, n));
  for (i in ids){
    x[i] = x[i]/U[i,i];
    cs = 0;
    for (j in (i+1):n){
      cs = cs + x[j]*U[i,j];
  }
  x[i] = x[i] - cs/U[i,i];
}
return x;
}
 
/* */
array[] vector predict_fullglgc_rng(vector y, array[] vector obsXb, array[] vector predXb, array[] vector obsCoords, array[] vector predCoords, array[] vector z1, vector gamma, vector sigma1, vector sigma2, vector lscale1, vector lscale2, vector tau, int nsize, int psize, int postsize){
    array[postsize] vector[psize] out;
    int nprint = postsize %/% 10;
    for(l in 1:postsize) {
      if(l%nprint == 0) print("Starts for posterior sample : ", l); 
      matrix[nsize,psize] C10 = gp_matern32_cov(obsCoords, predCoords, 1, lscale1[l]);
      matrix[nsize,psize] C20 = gp_matern32_cov(obsCoords, predCoords, 1, lscale2[l]);
      matrix[nsize,nsize] Ch1 = cholesky_decompose(add_diag(gp_matern32_cov(obsCoords, 1, lscale1[l]), rep_vector(1e-7,nsize)));
      matrix[nsize,nsize] Ch2 = cholesky_decompose(add_diag(gp_matern32_cov(obsCoords, 1, lscale2[l]), rep_vector(square(tau[l])*inv_square(sigma2[l]),nsize)));
      vector[nsize] res = y - obsXb[l] - gamma[l]*exp(z1[l]);
      for(i in 1:psize) {
        row_vector[nsize] c10 = to_row_vector(C10[1:nsize,i]);
        real m1 =  c10*mdivide_left_tri_upp(Ch1',mdivide_left_tri_low(Ch1, z1[l]));
        real v1 = square(sigma1[l])*(1 - dot_self(mdivide_left_tri_low(Ch1, c10')));
        real z10 = normal_rng(m1, sqrt(v1));
        row_vector[nsize] c20 = to_row_vector(C20[1:nsize,i]);
        real m2 =  predXb[l][i] + gamma[l] * exp(z10) + c20*mdivide_left_tri_upp(Ch2',mdivide_left_tri_low(Ch2, res));
        real v2 = square(sigma2[l])*(1 + square(tau[l])*inv_square(sigma2[l]) - dot_self(mdivide_left_tri_low(Ch2, c20')));
        out[l][i] = normal_rng(m2, sqrt(v2));
      }
    }
    return out;
  }
  
} // functions ends

data {
  int<lower=0> N;
  int<lower=0> P;
  vector[N] y;
  matrix[N,P] X;
  array[N] vector[2] coords;
  vector[P] mu_theta;
  matrix[P,P] V_theta;
  real<lower=0> a;
  real<lower=0> b;
  int<lower=0, upper=1> positive_skewness;
}

transformed data {
  vector[N] jitter = rep_vector(1e-9, N);
  cholesky_factor_cov[P] chol_V_theta = cholesky_decompose(V_theta);
  int skewness;
  if(positive_skewness==0){
    skewness = -1;
    } else {
      skewness = 1;
    }
}


parameters{
  vector[P] theta_std;
  real<lower = 0> abs_gamma;
  real<lower = 0> sigma1;
  real<lower = 0> sigma2;
  real<lower = 0> tau;
  real<lower = 0> ell1;
  real<lower = 0> ell2;
  vector[N] noise1;
}

transformed parameters {
  real gamma = skewness * abs_gamma;
  // implies : theta ~ multi_normal_cholesky(mu_theta, chol_V_theta);
  vector[P] theta = mu_theta + chol_V_theta * theta_std;
  }

model {
  matrix[N,N] C1 = gp_matern32_cov(coords, sigma1, ell1);
  matrix[N,N] C2 = gp_matern32_cov(coords, sigma2, ell2);
  vector[N] z1 = cholesky_decompose(add_diag(C1,jitter)) * noise1;
  matrix[N,N] L = cholesky_decompose(add_diag(C2, rep_vector(square(tau), N) + jitter));

  theta_std ~ std_normal();
  abs_gamma ~ std_normal();
  sigma1 ~ std_normal();
  sigma2 ~ std_normal();
  tau ~ std_normal();
  ell1 ~ inv_gamma(a,b);
  ell2 ~ inv_gamma(a,b);
  noise1 ~ std_normal();
  vector[N] mu = X * theta + gamma * exp(z1);
  y ~ multi_normal_cholesky(mu, L);
}

generated quantities{
  
}

> cmdstan_fit <- mod$sample(data = input, 
+                           chains = 4,
+                           parallel_chains = 4,
+                           iter_warmup = 1000,
+                           iter_sampling = 1000,
+                           adapt_delta = 0.99,
+                           max_treedepth = 15,
+                           step_size = 0.25)
Running MCMC with 4 parallel chains...

Chain 1 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 1 Exception: gp_matern32_cov: magnitude is inf, but must be positive finite! (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 95, column 2 to column 57)
Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 1 
Chain 2 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 2 Exception: gp_matern32_cov: magnitude is inf, but must be positive finite! (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 95, column 2 to column 57)
Chain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 2 
Chain 3 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 3 Exception: gp_matern32_cov: length scale is 0, but must be positive finite! (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 95, column 2 to column 57)
Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 3 
Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 3 Exception: gp_matern32_cov: length scale is 0, but must be positive finite! (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 95, column 2 to column 57)
Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 3 
Chain 4 Iteration:    1 / 2000 [  0%]  (Warmup) 
Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 3 Exception: gp_matern32_cov: magnitude is inf, but must be positive finite! (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 95, column 2 to column 57)
Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 3 
Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 3 Exception: cholesky_decompose: A is not symmetric. A[1,2] = -nan, but A[2,1] = -nan (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 97, column 2 to column 88)
Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 3 
Chain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
Chain 4 Exception: cholesky_decompose: Matrix m is not positive definite (in '/tmp/RtmpfTSGOk/model-3e0e995735da43.stan', line 97, column 2 to column 88)
Chain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,
Chain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.
Chain 4 
Chain 2 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 1 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 4 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 3 Iteration:  100 / 2000 [  5%]  (Warmup) 
Chain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) 
Chain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) 
Chain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) 
Chain 1 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 1 finished in 35766.3 seconds.
Chain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) 
Chain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) 
Chain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) 
Chain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) 
Chain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 2 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 2 finished in 44190.9 seconds.
Chain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) 
Chain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) 
Chain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) 
Chain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) 
Chain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) 
Chain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) 
Chain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) 
Chain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) 
Chain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) 
Chain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) 
Chain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) 
Chain 4 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 4 finished in 63174.5 seconds.
Chain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) 
Chain 3 Iteration: 2000 / 2000 [100%]  (Sampling) 
Chain 3 finished in 64868.0 seconds.

All 4 chains finished successfully.
Mean chain execution time: 51999.9 seconds.
Total execution time: 64870.0 seconds.

> elapsed_time <- cmdstan_fit$time()
> elapsed_time
$total
[1] 64869.98

$chains
  chain_id  warmup sampling   total
1        1 25591.1  10175.2 35766.3
2        2 23193.9  20997.0 44190.9
3        3 46858.9  18009.1 64868.0
4        4 28903.3  34271.2 63174.5

> elapsed_time$total/3600
[1] 18.01944
> 
> cmdstan_fit$cmdstan_diagnose()
Processing csv files: /tmp/RtmpfTSGOk/Full_GLGC_HN-202311260136-1-d11d31.csv, /tmp/RtmpfTSGOk/Full_GLGC_HN-202311260136-2-d11d31.csv, /tmp/RtmpfTSGOk/Full_GLGC_HN-202311260136-3-d11d31.csv, /tmp/RtmpfTSGOk/Full_GLGC_HN-202311260136-4-d11d31.csv

Checking sampler transitions treedepth.
Treedepth satisfactory for all transitions.

Checking sampler transitions for divergences.
No divergent transitions found.

Checking E-BFMI - sampler transitions HMC potential energy.
E-BFMI satisfactory.

Effective sample size satisfactory.

Split R-hat values satisfactory all parameters.

Processing complete, no problems detected.
> sampler_diag <- cmdstan_fit$sampler_diagnostics(format = "df")
> str(sampler_diag)
draws_df [4,000 × 9] (S3: draws_df/draws/tbl_df/tbl/data.frame)
 $ treedepth__  : num [1:4000] 7 7 7 7 7 7 7 7 7 7 ...
 $ divergent__  : num [1:4000] 0 0 0 0 0 0 0 0 0 0 ...
 $ energy__     : num [1:4000] 500 475 490 462 457 ...
 $ accept_stat__: num [1:4000] 0.976 0.965 0.983 0.888 0.997 ...
 $ stepsize__   : num [1:4000] 0.0328 0.0328 0.0328 0.0328 0.0328 ...
 $ n_leapfrog__ : num [1:4000] 127 127 127 127 127 127 127 127 127 127 ...
 $ .chain       : int [1:4000] 1 1 1 1 1 1 1 1 1 1 ...
 $ .iteration   : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...
 $ .draw        : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...
> 
> ## Posterior summaries
> pars <- c(paste0("theta[",1:P,"]"),"sigma1","sigma2","ell1","ell2","tau","gamma")
> pars_true_df <- tibble(variable = pars, true = c(theta,sigma1,sigma2,lscale1,lscale2,tau,gamma))
> fit_summary <- cmdstan_fit$summary(NULL, c("mean","sd","quantile50","quantile2.5","quantile97.5","rhat","ess_bulk","ess_tail"))
> fixed_summary <- inner_join(pars_true_df, fit_summary)
Joining with `by = join_by(variable)`
> fixed_summary %>% print(digits = 3)
# A tibble: 9 × 10
  variable  true   mean     sd  `50%` `2.5%` `97.5%`  rhat ess_bulk ess_tail
  <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>
1 theta[1]  5     4.02  0.671   4.02   2.71    5.35   1.00    1624.    2085.
2 theta[2] -1    -0.979 0.0254 -0.980 -1.03   -0.929  1.00    5821.    3221.
3 theta[3]  1     1.04  0.0270  1.04   0.981   1.09   1.00    6173.    3482.
4 sigma1    1     0.998 0.307   0.951  0.551   1.76   1.00    1117.    1826.
5 sigma2    1     1.01  0.282   0.970  0.582   1.67   1.00    1879.    2472.
6 ell1      0.5   0.545 0.131   0.527  0.335   0.847  1.00    1429.    2177.
7 ell2      0.5   0.600 0.203   0.562  0.323   1.11   1.00    1249.    2122.
8 tau       0.5   0.490 0.0193  0.490  0.454   0.529  1.00    5166.    2799.
9 gamma    -0.75 -1.03  0.497  -0.963 -2.17   -0.236  1.00    1389.    1556.
> 
> ## Posterior draws
> draws_df <- cmdstan_fit$draws(format = "df")
> draws_df
# A draws_df: 1000 iterations, 4 chains, and 514 variables
   lp__ theta_std[1] theta_std[2] theta_std[3] abs_gamma sigma1 sigma2  tau
1  -226         0.31        -0.98         1.06      0.79   0.98   1.45 0.49
2  -236         0.57        -0.96         1.00      1.52   0.66   0.92 0.48
3  -208         0.58        -0.97         1.04      1.17   0.70   1.00 0.49
4  -202         0.66        -1.00         0.99      1.98   0.77   0.94 0.47
5  -214         0.44        -0.98         1.08      0.36   1.28   1.00 0.49
6  -237         0.40        -0.95         1.02      0.30   1.62   1.19 0.47
7  -229         0.50        -1.03         1.02      1.14   1.04   0.80 0.52
8  -237         0.33        -0.98         1.04      0.44   1.35   0.86 0.50
9  -264         0.41        -0.98         1.04      0.17   1.30   0.75 0.49
10 -269         0.29        -0.98         1.04      0.18   1.27   0.88 0.49
# ... with 3990 more draws, and 506 more variables
# ... hidden reserved variables {'.chain', '.iteration', '.draw'}
> 
> library(bayesplot)
This is bayesplot version 1.10.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
> color_scheme_set("brewer-Spectral")
> mcmc_trace(draws_df,  pars = pars, facet_args = list(ncol = 3)) + facet_text(size = 15)
> 
> ## Recovery of random effect z1
> size_post_samples <- nrow(draws_df); size_post_samples
[1] 4000
> post_ell1 <- as_tibble(draws_df) %>% .$ell1; str(post_ell1)
 num [1:4000] 0.372 0.442 0.322 0.434 0.605 ...
> post_sigma1 <- as_tibble(draws_df) %>% .$sigma1; str(post_sigma1)
 num [1:4000] 0.976 0.658 0.698 0.77 1.277 ...
> post_noise1 <- as_tibble(draws_df) %>% select(starts_with("noise1[")) %>% as.matrix() %>% unname(); str(post_noise1)
 num [1:4000, 1:500] 0.467 1.373 0.569 0.297 1.572 ...
> post_z1 <- array(0, dim = c(size_post_samples,nsize)); str(post_z1)
 num [1:4000, 1:500] 0 0 0 0 0 0 0 0 0 0 ...
> obsDistMat <- fields::rdist(obsCoords)
> l <- 1
> for(l in 1:size_post_samples){
+   C1 <- matern32(d = obsDistMat, sigma = post_sigma1[l],  lscale = post_ell1[l])
+   post_z1[l,] <- drop(crossprod(chol(C1),post_noise1[l,]))
+ }
> str(post_z1)
 num [1:4000, 1:500] 0.456 0.903 0.397 0.229 2.007 ...
> 
> z1_summary <- tibble(z1 = obsZ1,
+                      post.mean = apply(post_z1, 2, mean),
+                      post.sd = apply(post_z1, 2, sd),
+                      post.q2.5 = apply(post_z1, 2, quantile2.5),
+                      post.q50 = apply(post_z1, 2, quantile50),
+                      post.q97.5 = apply(post_z1, 2, quantile97.5))
> z1_summary
# A tibble: 500 × 6
        z1 post.mean post.sd post.q2.5 post.q50 post.q97.5
     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>      <dbl>
 1  1.59       0.939   0.554   -0.0366    0.892      2.21 
 2  1.20       1.12    0.522    0.270     1.07       2.32 
 3 -0.0983     0.407   0.591   -0.732     0.387      1.60 
 4  1.20       0.756   0.545   -0.173     0.705      2.01 
 5  2.41       2.03    0.520    1.24      1.96       3.28 
 6  0.896      0.366   0.625   -0.923     0.371      1.60 
 7 -1.09      -0.953   0.704   -2.47     -0.903      0.365
 8 -1.09      -0.253   0.639   -1.55     -0.242      1.03 
 9  0.837     -0.457   0.870   -2.46     -0.343      0.942
10 -0.407     -0.794   0.859   -2.75     -0.699      0.674
# ℹ 490 more rows
> z1_summary %>% mutate(btw = between(z1, post.q2.5,post.q97.5)) %>% .$btw %>% mean()
[1] 1
> 
> save(elapsed_time, fixed_summary, draws_df, z1_summary, file = paste0(fpath,"ExactVsApproximateMethods/Full_DataSet2.RData"))
> 
> ##################################################################
> ## Independent prediction at each predictions sites
> ##################################################################
> source(paste0(fpath,"Rutilities/expose_cmdstanr_functions.R"))
> exsf <- expose_cmdstanr_functions(model_path = stan_file)
Warning message:
In readLines(temp_cpp_file) :
  incomplete final line found on '/tmp/RtmpfTSGOk/model-3e0e99686193c7.cpp'
> args(exsf$predict_fullglgc_rng)
function (y, obsXb, predXb, obsCoords, predCoords, z1, gamma, 
    sigma1, sigma2, lscale1, lscale2, tau, nsize, psize, postsize, 
    base_rng__ = <pointer: 0x17318380>, pstream__ = <pointer: 0x14ca1c9b9ce0>) 
NULL
> 
> size_post_samples <- nrow(draws_df); size_post_samples
[1] 4000
> psize <- nrow(prdCoords); psize
[1] 9500
> post_sigma1 <- as_tibble(draws_df) %>% .$sigma1; str(post_sigma1)
 num [1:4000] 0.976 0.658 0.698 0.77 1.277 ...
> post_sigma2 <- as_tibble(draws_df) %>% .$sigma2; str(post_sigma2)
 num [1:4000] 1.448 0.922 1.005 0.945 1.003 ...
> post_tau <- as_tibble(draws_df) %>% .$tau; str(post_tau)
 num [1:4000] 0.495 0.476 0.487 0.474 0.486 ...
> post_ell1 <- as_tibble(draws_df) %>% .$ell1; str(post_ell1)
 num [1:4000] 0.372 0.442 0.322 0.434 0.605 ...
> post_ell2 <- as_tibble(draws_df) %>% .$ell2; str(post_ell2)
 num [1:4000] 0.728 0.603 0.763 0.647 0.649 ...
> post_gamma <- as_tibble(draws_df) %>% .$gamma; str(post_gamma)
 num [1:4000] -0.787 -1.522 -1.171 -1.979 -0.359 ...
> post_theta <- as_tibble(draws_df) %>% select(starts_with("theta[")) %>% as.matrix() %>% unname(); str(post_theta)
 num [1:4000, 1:3] 3.28 4.13 4.16 4.4 3.7 ...
> str(post_z1)
 num [1:4000, 1:500] 0.456 0.903 0.397 0.229 2.007 ...
> 
> str(obsX)
 num [1:500, 1:3] 1 1 1 1 1 1 1 1 1 1 ...
> str(post_theta)
 num [1:4000, 1:3] 3.28 4.13 4.16 4.4 3.7 ...
> 
> obsXtheta <- t(sapply(1:size_post_samples, function(l) obsX %*% post_theta[l,])); str(obsXtheta)
 num [1:4000, 1:500] 3.39 4.25 4.27 4.53 3.81 ...
> prdXtheta <- t(sapply(1:size_post_samples, function(l) prdX %*% post_theta[l,])); str(prdXtheta)
 num [1:4000, 1:9500] 4.3 5.07 5.16 5.32 4.74 ...
> 
> str(exsf$my_gp_matern32_cov(x = lapply(1:nsize, function(i) obsCoords[i,]), y = lapply(1:psize, function(i) prdCoords[i,]), sigma = 1, lscale = 1))
 num [1:500, 1:9500] 0.441 0.401 0.155 0.324 0.446 ...
> 
> post_ypred <- exsf$predict_fullglgc_rng(
+   y = obsY, 
+   obsXb = lapply(1:size_post_samples, function(i) obsXtheta[i,]), 
+   predXb = lapply(1:size_post_samples, function(i) prdXtheta[i,]), 
+   obsCoords = lapply(1:nsize, function(i) obsCoords[i,]), 
+   predCoords = lapply(1:psize, function(i) prdCoords[i,]), 
+   z1 = lapply(1:size_post_samples, function(i) post_z1[i,]), 
+   gamma = post_gamma, 
+   sigma1 = post_sigma1, 
+   sigma2 = post_sigma2,
+   lscale1 = post_ell1,
+   lscale2 = post_ell2, 
+   tau = post_tau, 
+   nsize = nsize, 
+   psize = psize,
+   postsize = size_post_samples)
Starts for posterior sample : 400
Starts for posterior sample : 800
Starts for posterior sample : 1200
Starts for posterior sample : 1600
Starts for posterior sample : 2000
Starts for posterior sample : 2400
Starts for posterior sample : 2800
Starts for posterior sample : 3200
Starts for posterior sample : 3600
Starts for posterior sample : 4000
> 
> ypred_draws <- do.call(rbind,post_ypred); str(ypred_draws)
 num [1:4000, 1:9500] 3.37 3.77 2.84 3.25 3.07 ...
> pred_summary <- tibble(
+   post.mean = apply(ypred_draws, 2, mean),
+   post.sd = apply(ypred_draws, 2, sd),
+   post.q2.5 = apply(ypred_draws, 2, quantile2.5),
+   post.q50 = apply(ypred_draws, 2, quantile50),
+   post.q97.5 = apply(ypred_draws, 2, quantile97.5),
+   y = prdY)
> pred_summary
# A tibble: 9,500 × 6
   post.mean post.sd post.q2.5 post.q50 post.q97.5     y
       <dbl>   <dbl>     <dbl>    <dbl>      <dbl> <dbl>
 1     3.02    0.717     1.55     3.02        4.38  4.04
 2     2.40    0.693     0.983    2.40        3.75  2.07
 3     3.42    0.660     2.11     3.43        4.70  2.38
 4     2.42    0.633     1.19     2.44        3.69  2.25
 5     0.465   0.616    -0.774    0.472       1.63  1.78
 6     2.42    0.598     1.26     2.41        3.60  2.54
 7     3.63    0.596     2.45     3.63        4.77  4.36
 8     2.66    0.602     1.48     2.67        3.82  2.96
 9     2.60    0.624     1.37     2.62        3.82  4.10
10     0.826   0.637    -0.432    0.837       2.06  1.80
# ℹ 9,490 more rows
> mean(pred_summary[,"y"]>pred_summary[,"post.q2.5"] & pred_summary[,"y"]<pred_summary[,"post.q97.5"])
[1] 0.9394737
> 
> ## Computation for scoring rules
> 
> library(scoringRules)
> ES <- es_sample(y = prdY, dat = t(ypred_draws)); ES
[1] 41.77749
> #VS0.25 <- vs_sample(y = prdY, dat = t(ypred_draws), p = 0.25); VS0.25
> logs <- mean(logs_sample(y = prdY, dat = t(ypred_draws))); logs
[1] 0.9018054
> CRPS <- mean(crps_sample(y = prdY, dat = t(ypred_draws))); CRPS
[1] 0.3377469
> 
> scores_df <- pred_summary %>% 
+   mutate(intervals = scoringutils::interval_score(true_values = y, lower = post.q2.5, upper = post.q50, interval_range = 0.95)) %>%
+   mutate(btw = between(y,post.q2.5, post.q97.5)) %>%
+   mutate(error = y - post.q50) %>%
+   summarise(MAE = sqrt(mean(abs(error))), RMSE = sqrt(mean(error^2)), CVG = mean(btw),
+             IS = mean(intervals)) %>%
+   mutate(ES = ES, logs = logs, CRPS = CRPS,  `Elapsed Time` = elapsed_time$total, Method = "Full_DataSet2") %>%
+   select(Method,MAE,RMSE,CVG,CRPS,IS,ES,logs,`Elapsed Time`)
> scores_df
# A tibble: 1 × 9
  Method          MAE  RMSE   CVG  CRPS    IS    ES  logs `Elapsed Time`
  <chr>         <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>          <dbl>
1 Full_DataSet2 0.691 0.607 0.939 0.338 0.784  41.8 0.902         64870.
> 
> save(elapsed_time, fixed_summary, draws_df, z1_summary, pred_summary, scores_df, file = paste0(fpath,"ExactVsApproximateMethods/Full_DataSet2.RData"))
> 
